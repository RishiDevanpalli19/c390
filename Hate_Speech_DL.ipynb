{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FJYSsBZz5JCO"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCxayj3NCs49",
        "outputId": "2b2bda15-b20d-4776-8768-a42deb1f2d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcHHXT7tDUCZ"
      },
      "outputs": [],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BadxYuaZFiJA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ixPuDPDfcA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow.keras.backend as K\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "from transformers import *\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(tf.__version__)\n",
        "\n",
        "import shutil\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMa_e2dxGFTM"
      },
      "source": [
        "reading and preparing the  dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neCVLy3SGMFg"
      },
      "outputs": [],
      "source": [
        "#PATH = '../input/hate-speech-and-offensive-language-dataset/'\n",
        "PATH = '/content/labeled_data.csv'\n",
        "df = pd.read_csv(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m-MsPFCGsN8"
      },
      "outputs": [],
      "source": [
        "nRowsRead = None # specify 'None' if want to read whole file\n",
        "# labeled_data.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
        "df0 = pd.read_csv('/content/labeled_data.csv', delimiter=',', nrows = nRowsRead)\n",
        "df0.dataframeName = 'labeled_data.csv'\n",
        "nRow, nCol = df0.shape\n",
        "print('There are {} rows and {} columns'.format(nRow, nCol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaTHGA1zG2n_"
      },
      "outputs": [],
      "source": [
        "df0.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7WNS1qLG-XM"
      },
      "outputs": [],
      "source": [
        "#Doing some adjustments\n",
        "\n",
        "c=df0['class']\n",
        "df0.rename(columns={'tweet' : 'text',\n",
        "                   'class' : 'category'}, \n",
        "                    inplace=True)\n",
        "a=df0['text']\n",
        "b=df0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n",
        "\n",
        "df= pd.concat([a,b,c], axis=1)\n",
        "df.rename(columns={'class' : 'label'}, \n",
        "                    inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPMR3eaIG_Os"
      },
      "outputs": [],
      "source": [
        "# Grouping data by label\n",
        "df.groupby('label').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE_1zh3OHMoG"
      },
      "outputs": [],
      "source": [
        "# this is unbalanced dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExsjYDULHUEE"
      },
      "outputs": [],
      "source": [
        "hate, ofensive, neither = np.bincount(df['label'])\n",
        "total = hate + ofensive + neither\n",
        "print('Examples:\\n    Total: {}\\n    hate: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, hate, 100 * hate / total))\n",
        "print('Examples:\\n    Total: {}\\n    Ofensive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, ofensive, 100 * ofensive / total))\n",
        "print('Examples:\\n    Total: {}\\n    Neither: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, neither, 100 * neither / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O405_s_1HYZo"
      },
      "outputs": [],
      "source": [
        "X_train_, X_test, y_train_, y_test = train_test_split(\n",
        "    df.index.values,\n",
        "    df.label.values,\n",
        "    test_size=0.10,\n",
        "    random_state=42,\n",
        "    stratify=df.label.values,    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rITvflZEHl61"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df.loc[X_train_].index.values,\n",
        "    df.loc[X_train_].label.values,\n",
        "    test_size=0.10,\n",
        "    random_state=42,\n",
        "    stratify=df.loc[X_train_].label.values,  \n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmFvBmw3HwpN"
      },
      "outputs": [],
      "source": [
        "df['data_type'] = ['not_set']*df.shape[0]\n",
        "df.loc[X_train, 'data_type'] = 'train'\n",
        "df.loc[X_val, 'data_type'] = 'val'\n",
        "df.loc[X_test, 'data_type'] = 'test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th4woJKZH4_o"
      },
      "outputs": [],
      "source": [
        "df.groupby(['category', 'label', 'data_type']).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O_BQsGfH-w8"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4jKH8XeIBrZ"
      },
      "outputs": [],
      "source": [
        "df_train = df.loc[df[\"data_type\"]==\"train\"]\n",
        "df_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MU4ezTzIHQn"
      },
      "outputs": [],
      "source": [
        "df_val = df.loc[df[\"data_type\"]==\"val\"]\n",
        "df_val.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXXnC5HLIH4I"
      },
      "outputs": [],
      "source": [
        "df_test = df.loc[df[\"data_type\"]==\"test\"]\n",
        "df_test.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j3e5vwqINMx"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lnYqoo6INaD"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"RT\")\n",
        "\n",
        "print(type(STOPWORDS))\n",
        "\n",
        "import random\n",
        "\n",
        "def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n",
        "    h = 344\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stopwords,\n",
        "                          max_words=200,\n",
        "                          max_font_size=60, \n",
        "                          random_state=42\n",
        "                         ).generate(str(df.loc[df[\"category\"]==\"offensive_language\"].text))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n",
        "           interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mve6PwVXINpI"
      },
      "outputs": [],
      "source": [
        "def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n",
        "    h = 20\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stopwords,\n",
        "                          max_words=200,\n",
        "                          max_font_size=60, \n",
        "                          random_state=42\n",
        "                         ).generate(str((df.loc[df[\"category\"]==\"neither\"].text)))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n",
        "           interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szTTQ0WINxO"
      },
      "outputs": [],
      "source": [
        "stopwords.add(\"Name\")\n",
        "\n",
        "def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n",
        "    h = 180\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stopwords,\n",
        "                          max_words=200,\n",
        "                          max_font_size=60, \n",
        "                          random_state=42\n",
        "                         ).generate(str((df.loc[df[\"category\"]==\"hate_speech\"].text)))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n",
        "           interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPjhLxP-IN75"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((df_train.text.values, df_train.label.values))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((df_val.text.values, df_val.label.values))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((df_test.text.values, df_test.label.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op5w_hwpKWWs"
      },
      "outputs": [],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ9gNF1TKXnS"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.shuffle(len(df_train)).batch(32, drop_remainder=False)\n",
        "train_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W4O03ctKXyu"
      },
      "outputs": [],
      "source": [
        "val_ds = val_ds.shuffle(len(df_val)).batch(32, drop_remainder=False)\n",
        "val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgkkQgrqKX6r"
      },
      "outputs": [],
      "source": [
        "test_ds = test_ds.shuffle(len(df_test)).batch(32, drop_remainder=False)\n",
        "test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytp-bYQUKYEK"
      },
      "outputs": [],
      "source": [
        "#print some tweets\n",
        "for feat, targ in train_ds.take(1):\n",
        "  print ('Features: {}, Target: {}'.format(feat, targ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOCIOt4fKYLc"
      },
      "outputs": [],
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
        "#bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/1',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JykqRlWQL2Bi"
      },
      "outputs": [],
      "source": [
        "##preprocessing model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9rKdgYaMFn1"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtveyuwAMJ3s"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(1):\n",
        "    tweet = text_batch.numpy()[i]\n",
        "    print(f'Tweet: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label}')\n",
        "\n",
        "text_test = ['this is such an amazing movie!']\n",
        "text_test = [tweet]\n",
        "\n",
        "\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtXx3R5qMPOD"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpX4JDFHMT1U"
      },
      "outputs": [],
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTY0yIG_MXyH"
      },
      "outputs": [],
      "source": [
        "#Techniques to deal with unbalanced data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PyZne6RMiCz"
      },
      "outputs": [],
      "source": [
        "weight_for_0 = (1 / hate)*(total)/3.0 \n",
        "weight_for_1 = (1 / ofensive)*(total)/3.0\n",
        "weight_for_2 = (1 / neither)*(total)/3.0\n",
        "\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJrwtWi3MsoQ"
      },
      "outputs": [],
      "source": [
        "#Set the correct initial bias¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY0UJPceMxdG"
      },
      "outputs": [],
      "source": [
        "#initial_output_bias = np.array([3.938462, 6.535164, 5.])\n",
        "initial_output_bias = np.array([3.938462, 15, 5.])\n",
        "initial_output_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qPvlJ_NM1pi"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(output_bias=None):\n",
        "    if output_bias is not None:\n",
        "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "        #print(output_bias)\n",
        "        \n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs['pooled_output']\n",
        "    net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n",
        "    net = tf.keras.layers.Dropout(0.2)(net)\n",
        "#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "    net = tf.keras.layers.Dense(3, activation=\"softmax\", name='classifier', bias_initializer=output_bias)(net)\n",
        "    \n",
        "    return tf.keras.Model(text_input, net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx2wsk6xN1up"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model(output_bias=initial_output_bias)\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89vZqYfhN1zB"
      },
      "outputs": [],
      "source": [
        "classifier_model.get_weights()[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zazvbPlbN_E1"
      },
      "outputs": [],
      "source": [
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYYRYyZIOZyr"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCBEQRBmOaKX"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "#metrics = tf.metrics.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP7HlIVUOaNz"
      },
      "outputs": [],
      "source": [
        "epochs = 18\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7CIUzSDOaWn"
      },
      "outputs": [],
      "source": [
        "#  classifier_model.compile(optimizer=optimizer,\n",
        "#                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#                           metrics=['accuracy'])\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1T7rr5PWLrs"
      },
      "outputs": [],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}') \n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs,\n",
        "                               # The class weights go here\n",
        "                               class_weight=class_weight\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIspIwsSQLN8"
      },
      "outputs": [],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}') \n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs,\n",
        "                               # The class weights go here\n",
        "                               class_weight=class_weight\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQToTwWHWMuD"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ukbZR7dQLYG"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "# acc = history_dict['binary_accuracy']\n",
        "# val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1cQSJW4QLbJ"
      },
      "outputs": [],
      "source": [
        "###Export for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VINtqNu7QLfL"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'mpl_hate_speech'\n",
        "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "classifier_model.save(saved_model_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgDaQdHOQLiz"
      },
      "outputs": [],
      "source": [
        "pickle is used to sav the model "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "65db5f900325680cf3b91ed40f7c1ad204e8ff523c77aa3a8b1ebf49ea8063bb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
